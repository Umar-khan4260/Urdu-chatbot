# create_vocab.py
import pickle
import pandas as pd
import unicodedata
import re
from collections import Counter

def normalize_urdu(text):
    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')
    text = re.sub(r'[أإ]', 'ا', text)
    text = re.sub(r'ے', 'ی', text)
    text = re.sub(r'\\s+', ' ', text).strip()
    return text

def tokenize(text):
    return text.split()

# Load your dataset
df = pd.read_csv('final_main_dataset.tsv', sep='\\t', on_bad_lines='skip')
sentences = df['sentence'].dropna().tolist()

# Process sentences
normalized_sentences = [normalize_urdu(s) for s in sentences]
tokenized_sentences = [tokenize(s) for s in normalized_sentences]

# Create pairs and build vocabulary
pairs = []
for i in range(len(tokenized_sentences) - 1):
    pairs.append((tokenized_sentences[i], tokenized_sentences[i+1]))

def build_vocab(pairs):
    counter = Counter()
    for pair in pairs:
        counter.update(pair[0])
        counter.update(pair[1])
    vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + [word for word, _ in counter.most_common()]
    word_to_idx = {word: idx for idx, word in enumerate(vocab)}
    return vocab, word_to_idx

vocab, word_to_idx = build_vocab(pairs)

# Save vocabulary
vocab_data = {
    'vocab': vocab,
    'word_to_idx': word_to_idx
}

with open('vocab.pkl', 'wb') as f:
    pickle.dump(vocab_data, f)

print(f"Vocabulary saved with {len(vocab)} words")
